{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOfzSVsnM0MdSL/Y5069ox+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geonextgis/End-to-End-Deep-Learning/blob/main/02_CNN/03_Padding_and_Strides_in_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Padding and Strides in CNN**"
      ],
      "metadata": {
        "id": "AEzY8_kXTtiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is Padding?**\n",
        "\n",
        "Padding is a technique used to preserve spatial information during the convolutional and pooling operations. It involves adding extra pixels (usually with a value of zero) around the borders of an input feature map or image.\n",
        "\n",
        "The main purposes of padding are:\n",
        "\n",
        "1. **Preserving Spatial Information:**\n",
        "   - Without padding, the spatial dimensions of the feature map decrease with each convolutional layer, potentially leading to a significant reduction in information at the edges.\n",
        "   - Padding helps maintain the spatial size, ensuring that information near the borders is given proper consideration.\n",
        "\n",
        "2. **Mitigating the Loss of Information:**\n",
        "   - In the absence of padding, the pixels at the edges of the feature map are involved in fewer convolution operations, leading to a loss of information.\n",
        "   - Padding ensures that each pixel in the input has the opportunity to be the center of the receptive field for convolutional filters.\n",
        "\n",
        "3. **Handling Stride and Filter Size:**\n",
        "   - Padding becomes especially useful when using larger filter sizes or strides greater than 1. Without padding, the spatial size reduction becomes more pronounced."
      ],
      "metadata": {
        "id": "H2W2Vx7uT29o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://miro.medium.com/v2/resize:fit:1358/1*D6iRfzDkz-sEzyjYoVZ73w.gif\" width=\"70%\"></center>"
      ],
      "metadata": {
        "id": "FsR6b6F7T7bq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Types of Padding in Keras**\n",
        "In Keras, a popular deep learning library, you can specify different types of padding for convolutional layers. The main types of padding available in Keras are:\n",
        "\n",
        "1. **Valid Padding (No Padding):**\n",
        "   - This is the default setting in Keras.\n",
        "   - No padding is added to the input feature map.\n",
        "   - The convolution operation is applied only to the valid part of the input.\n",
        "\n",
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/7/78/Valid-padding-convolution.gif\" width=\"30%\"></center>\n",
        "\n",
        "2. **Same Padding (Zero Padding):**\n",
        "   - Padding is added to the input feature map to ensure that the spatial dimensions of the output feature map remain the same as the input.\n",
        "   - The padding is distributed evenly on all sides.\n",
        "   - Useful for preserving spatial information and handling larger filter sizes.\n",
        "\n",
        "<center><img src=\"https://miro.medium.com/v2/resize:fit:679/1*SsKCClCa9xVxIoaocVY6Ww.gif\" width=\"60%\"></center>"
      ],
      "metadata": {
        "id": "Cc1fkL9GXE5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculation of Feature Map Size**\n",
        "If the stride $(\\text{Stride})$ is set to 1 (meaning no skipping of pixels during the convolution), the formula for calculating the feature map size after padding simplifies further. For \"same\" padding, the formula becomes:\n",
        "\n",
        "$$\\text{Output Size} = {{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size} + 1}}$$\n",
        "\n",
        "Here, the terms are the same as in the previous formula:\n",
        "\n",
        "- $\\text{Output Size}$: The size of the feature map after the convolution operation with padding and stride set to 1.\n",
        "- $\\text{Input Size}$: The size of the input (or previous layer's feature map).\n",
        "- $\\text{Filter Size}$: The size of the convolutional filter (kernel).\n",
        "- $\\text{Padding}$: The number of zero-padding pixels added to the input on each side."
      ],
      "metadata": {
        "id": "QmzlhxiSY0Ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementation of Padding in Keras**"
      ],
      "metadata": {
        "id": "XRvWFJMjZuPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import Required Libraries**"
      ],
      "metadata": {
        "id": "X_VhDSljbGN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Conv2D, Dense, Flatten\n",
        "from keras.datasets import mnist\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNfPCR7sZzlY",
        "outputId": "7f434a71-6d4e-4769-9e59-0c783c61adb9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Read the Data**"
      ],
      "metadata": {
        "id": "kiB8qW3zbJ4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "oiGjkWo3bOrI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Build the Model Architecture with `valid` Padding**"
      ],
      "metadata": {
        "id": "GpyQH4m_bj6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model architecture with 'valid' padding in the convolution layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), padding=\"valid\", activation=\"relu\", input_shape=(28, 28, 1)))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), padding=\"valid\", activation=\"relu\"))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), padding=\"valid\", activation=\"relu\"))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "W5XaAUCmbvw8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model's summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9B2_Kr_h4Ls",
        "outputId": "b023cf54-25a4-4ab0-8357-72ecf777166b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 22, 22, 32)        9248      \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 15488)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               1982592   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2002698 (7.64 MB)\n",
            "Trainable params: 2002698 (7.64 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Build the Model Architecture with `same/zero` Padding**"
      ],
      "metadata": {
        "id": "46nf_dlci9sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model architecture with 'zero' padding in the convolution layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", input_shape=(28, 28, 1)))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "NhUEadZajEXr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model's summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR1cxM2GkKaL",
        "outputId": "d22b576a-47eb-433f-8881-4890795b9cee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               3211392   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3231498 (12.33 MB)\n",
            "Trainable params: 3231498 (12.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is Strides?**\n",
        "In the context of CNNs, \"strides\" refer to the step size or the number of pixels the convolutional filter (kernel) moves at each step during the convolution operation. The stride parameter determines the distance between consecutive applications of the filter to the input, influencing the spatial dimensions of the output feature map. `Strided convolution` involves using a stride value greater than 1, meaning that the convolutional filter moves more than one pixel at a time while scanning the input.\n",
        "\n",
        "Key points about strides:\n",
        "\n",
        "1. **Stride Value:**\n",
        "   - Strides are usually set as positive integers.\n",
        "   - Common values are 1, indicating that the filter moves one pixel at a time, and 2, indicating that the filter moves two pixels at a time.\n",
        "   - Larger stride values result in a more aggressive reduction of the spatial dimensions.\n",
        "\n",
        "2. **Effect on Output Size:**\n",
        "   - Increasing the stride reduces the spatial dimensions of the output feature map.\n",
        "   - Smaller strides lead to larger feature maps but may increase computational complexity.\n",
        "\n",
        "3. **Strides and Subsampling:**\n",
        "   - Strides can be used to achieve subsampling or down-sampling by skipping pixels during the convolution.\n",
        "   - Subsampling can be beneficial for reducing the computational load and focusing on important features."
      ],
      "metadata": {
        "id": "-GlP0wjpkaiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example of a Convolution Operation when the Stride is set to 2:**\n",
        "<br>\n",
        "<center><img src=\"https://miro.medium.com/v2/resize:fit:679/0*0LMdR2rvJAlRHC3m.gif\" width=\"40%\"></center>"
      ],
      "metadata": {
        "id": "znt1YU9HntY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculation of Feature Map Size**\n",
        "The effect of strides on the output size can be described by the following formula:\n",
        "\n",
        "$$\\text{Output Size} = \\frac{\\text{Input Size} + 2 \\times \\text{Padding} - \\text{Filter Size}}{{\\text{Stride}}} + 1$$\n",
        "\n",
        "Here are the terms in the formula:\n",
        "\n",
        "- $\\text{Output Size}$: The size of the feature map after the convolution operation.\n",
        "- $\\text{Input Size}$: The size of the input (or previous layer's feature map).\n",
        "- $\\text{Filter Size}$: The size of the convolutional filter (kernel).\n",
        "- $\\text{Padding}$: The number of zero-padding pixels added to the input on each side.\n",
        "- $\\text{Stride}$: The step size or the number of pixels the filter moves at each step during convolution."
      ],
      "metadata": {
        "id": "DnNqCyknkgim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Why Strides are required?**\n",
        "Strides in convolutional neural networks (CNNs) are required for several reasons:\n",
        "\n",
        "1. **Downsampling and Efficiency:**\n",
        "   - Strides enable downsampling of the input, reducing the spatial dimensions of the feature maps.\n",
        "   - Downsampling is crucial for efficiency, reducing computational complexity and memory requirements.\n",
        "\n",
        "2. **Feature Extraction:**\n",
        "   - Larger strides skip pixels during convolution, allowing the network to focus on more significant features and patterns.\n",
        "   - This can be beneficial for capturing high-level features and reducing the spatial size of the feature maps.\n",
        "\n",
        "3. **Control Over Model Complexity:**\n",
        "   - Strides provide a way to control the complexity of the model by influencing the spatial dimensions of the feature maps.\n",
        "   - They allow practitioners to balance between capturing fine-grained details and computational efficiency.\n",
        "\n",
        "In summary, strides are essential for controlling the trade-off between computational efficiency and feature representation in CNNs. They allow practitioners to tailor the network architecture to the specific requirements of the task at hand, ensuring effective feature extraction and model efficiency."
      ],
      "metadata": {
        "id": "0fKWpnHYpGey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementation of Strides in Keras**"
      ],
      "metadata": {
        "id": "Rnwu6HhxpczD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model architecture with a (2, 2) strides in the convolution layers\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), strides=(2,2), padding=\"same\", activation=\"relu\", input_shape=(28, 28, 1)))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), strides=(2,2), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), strides=(2,2), padding=\"same\", activation=\"relu\"))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "-S7iEUyupiYd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the model's summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIsVx4g5qWvD",
        "outputId": "58a35826-4714-432a-9b20-d3c0dc219be0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_9 (Conv2D)           (None, 14, 14, 32)        320       \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 7, 7, 32)          9248      \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 4, 4, 32)          9248      \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               65664     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 85770 (335.04 KB)\n",
            "Trainable params: 85770 (335.04 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}