{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOXuOJh+TbfQcBaFC+MeQaU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geonextgis/End-to-End-Deep-Learning/blob/main/02_CNN/02_Convolution_Operation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convolution Operation**"
      ],
      "metadata": {
        "id": "1QNzmwG-cWgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "Convolutional Neural Networks (CNNs) consist of multiple layers, each designed to perform specific operations to extract hierarchical features from input data, especially images. The key layers in a CNN include:\n",
        "\n",
        "1. **Input Layer:**\n",
        "   - Represents the raw data, often an image with pixel values.\n",
        "   - The size of the input layer corresponds to the dimensions of the input data, such as the height, width, and number of color channels.\n",
        "\n",
        "2. **Convolutional Layer:**\n",
        "   - Applies convolutional operations to the input data using filters or kernels.\n",
        "   - Each filter captures specific features or patterns within local receptive fields.\n",
        "   - Activations from this layer form feature maps that represent learned features.\n",
        "\n",
        "3. **Pooling (Subsampling) Layer:**\n",
        "   - Performs downsampling to reduce spatial dimensions and computational complexity.\n",
        "   - Common pooling operations include max pooling or average pooling, which retain the most salient features within local regions.\n",
        "\n",
        "4. **Fully Connected (Dense) Layer:**\n",
        "   - Neurons in this layer are connected to all neurons from the previous layer.\n",
        "   - Transforms high-level features into predictions or class scores.\n",
        "   - Commonly used in the final layers of the network.\n",
        "\n",
        "5. **Output Layer:**\n",
        "   - Produces the final output of the network.\n",
        "   - The number of neurons in this layer corresponds to the number of classes in a classification task.\n",
        "   - The activation function is chosen based on the nature of the task (e.g., softmax for classification)."
      ],
      "metadata": {
        "id": "TdRa-xTCcfUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://editor.analyticsvidhya.com/uploads/89175cnn_banner.png\" width=\"75%\"></center>"
      ],
      "metadata": {
        "id": "zpk2ikPsdjv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Basics of Image**\n",
        "In CNNs, images are fundamental inputs that undergo processing through various layers to extract hierarchical features. The nature of the image, whether grayscale or RGB (Red, Green, Blue), affects the input dimensions and the way the network processes the information.\n",
        "\n",
        "#### **Grayscale Images:**\n",
        "\n",
        "1. **Representation:**\n",
        "   - Grayscale images are represented using a single channel, where each pixel has a single intensity value (typically ranging from 0 to 255).\n",
        "   - In CNNs, a grayscale image is usually treated as a 2D matrix, where each element represents the intensity of a pixel.\n",
        "\n",
        "2. **Input Dimensions:**\n",
        "   - For a grayscale image of size H x W, the input tensor to the CNN would have dimensions (H, W, 1).\n",
        "   - The third dimension (1) signifies the single channel.\n",
        "\n",
        "#### **RGB Images:**\n",
        "\n",
        "1. **Representation:**\n",
        "   - RGB images are represented using three channels (Red, Green, Blue), where each channel represents the intensity of a specific color.\n",
        "   - Each pixel has three intensity values, forming a 3D matrix.\n",
        "\n",
        "2. **Input Dimensions:**\n",
        "   - For an RGB image of size H x W, the input tensor to the CNN would have dimensions (H, W, 3).\n",
        "   - The three channels correspond to Red, Green, and Blue."
      ],
      "metadata": {
        "id": "vXDpyAA_eUfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://www.baeldung.com/wp-content/uploads/sites/4/2022/09/NumericalRep.png\" width=\"60%\"></center>"
      ],
      "metadata": {
        "id": "fNVRCFZgewvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Edge Detection (Convolutional Operation)**\n",
        "Edge detection is a fundamental operation in image processing and computer vision that aims to identify boundaries within an image. One common approach to edge detection involves using convolutional filters, such as the Sobel filters, to highlight changes in intensity that correspond to edges. The Sobel filters are particularly effective for detecting edges in both the vertical and horizontal directions."
      ],
      "metadata": {
        "id": "6SoqTCEtfEvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example of a Vertical Edge Detection**\n",
        "\n",
        "<center><img src=\"https://media5.datahacker.rs/2018/11/conc_3_1.png\" width=\"70%\"></center>"
      ],
      "metadata": {
        "id": "2gQgUNUyg_N_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Demo:**\n",
        "[Click on this link.](https://deeplizard.com/resource/pavq7noze2)\n",
        "\n",
        "<center><img src=\"https://deeplizard.com/assets/jpg/a74bc5d5.jpg\" width=\"60%\"></center>"
      ],
      "metadata": {
        "id": "b78GVQr1hf6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculation of Feature Map Size**\n",
        "If the stride is set to 1 (meaning no skipping of pixels during the convolution), the formula for calculating the feature map size after a convolution operation simplifies to:\n",
        "\n",
        "$$ \\text{Output Size} = \\text{Input Size} - \\text{Filter Size} + 1 $$\n",
        "\n",
        "Here, the terms are:\n",
        "\n",
        "- $\\text{Output Size}$: The size of the feature map after the convolution operation.\n",
        "- $\\text{Input Size}$: The size of the input (or previous layer's feature map).\n",
        "- $\\text{Filter Size}$: The size of the convolutional filter (kernel).\n",
        "\n",
        "Example:\n",
        "$\\text{Output Size} = \\text{Input Size} - \\text{Filter Size} + 1 $\n",
        "\n",
        "Suppose you have a grayscale image with an input size of 28x28 and a filter size of 3x3:\n",
        "\n",
        "$\\text{Output Size} = 28 - 3 + 1 = 26 $\n",
        "\n",
        "So, in this case, the feature map size after the convolution operation would be 26x26. If you are not using any stride $\\text{Stride} = 1 $, this simplified formula is applicable."
      ],
      "metadata": {
        "id": "OAblCrzbIkRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/70_blog_image_3.png\" width=\"70%\"></center>"
      ],
      "metadata": {
        "id": "oYqDtjBQJNp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Working with RGB Images**"
      ],
      "metadata": {
        "id": "NH_CBC_VQKuJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convolution Operation on RGB Images:**\n",
        "<center><img src = \"https://media5.datahacker.rs/2018/11/06_04.png\" width=\"70%\"></center>"
      ],
      "metadata": {
        "id": "eUcZBf7NRbyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applying Multiple Filters on same RGB Image:**\n",
        "<br>\n",
        "<center><img src=\"https://media5.datahacker.rs/2018/11/06_09.png\" width=\"70%\"></center>"
      ],
      "metadata": {
        "id": "6aV6k9nAR56l"
      }
    }
  ]
}